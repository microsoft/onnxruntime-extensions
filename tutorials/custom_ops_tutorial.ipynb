{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Using Custom Ops with TF2ONNX\n",
    "\n",
    "The custom ops framework lets you define new ONNX operators in Python or C++ and load them into ORT.  This makes it possible to convert and run TF models with ops that have no current ONNX equivalent.  The framework also serves as a place for sharing custom op definitions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Case 1: Converting a TF model using an existing custom op\n",
    "\n",
    "First let's create a model that requires a custom op that is already defined in the custom ops framework"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModule1(tf.Module):\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        # Regex replacement isn't supported by ONNX, but there is a custom op definition\n",
    "        x_ = tf.strings.regex_replace(x, \" \", \"_\", replace_global=True)\n",
    "        return x_\n",
    "\n",
    "module1 = CustomModule1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Hello_world!'], dtype=object)>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "module1(tf.constant([\"Hello world!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model1\\assets\n"
     ]
    }
   ],
   "source": [
    "concrete_fn1 = module1.__call__.get_concrete_function(tf.TensorSpec([None], tf.string))\n",
    "tf.saved_model.save(module1, \"saved_model1\", signatures=concrete_fn1)"
   ]
  },
  {
   "source": [
    "### Identifying unsupported ops\n",
    "\n",
    "If a model has unsupported ops, tf2onnx will still convert it, but the unsupported ops will be left in the graph unchanged. An error message will list the unsupported ops."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-25 22:57:03.368651: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n2020-11-25 22:57:03.368980: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2020-11-25 22:57:06,057 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n2020-11-25 22:57:06.086128: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n2020-11-25 22:57:06.086632: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n2020-11-25 22:57:06.093257: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: MININT-44293CS\n2020-11-25 22:57:06.093781: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: MININT-44293CS\n2020-11-25 22:57:06.094460: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2020-11-25 22:57:06.103604: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c3ca9cfeb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-11-25 22:57:06.103907: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-11-25 22:57:06,143 - INFO - Signatures found in model: [serving_default].\n2020-11-25 22:57:06,144 - WARNING - '--signature_def' not specified, using first signature: serving_default\n2020-11-25 22:57:06.146296: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n2020-11-25 22:57:06.146765: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n2020-11-25 22:57:06.152370: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n2020-11-25 22:57:06.152555: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 6 nodes (3), 5 edges (3), time = 0.389ms.\n2020-11-25 22:57:06.152655: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.01ms.\nWARNING:tensorflow:From c:\\Users\\tomwi\\OneDrive - Microsoft\\ONNX\\tensorflow-onnx\\tf2onnx\\tf_loader.py:492: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\n2020-11-25 22:57:06,171 - WARNING - From c:\\Users\\tomwi\\OneDrive - Microsoft\\ONNX\\tensorflow-onnx\\tf2onnx\\tf_loader.py:492: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\n2020-11-25 22:57:06.173273: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n2020-11-25 22:57:06.173738: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n2020-11-25 22:57:06.178334: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n2020-11-25 22:57:06.178534: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   constant_folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.451ms.\n2020-11-25 22:57:06.178790: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.007ms.\n2020-11-25 22:57:06.179016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   constant_folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.079ms.\n2020-11-25 22:57:06.179205: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\n2020-11-25 22:57:06,183 - INFO - Using tensorflow=2.3.1, onnx=1.7.0, tf2onnx=1.8.0/240803\n2020-11-25 22:57:06,183 - INFO - Using opset <onnx, 8>\n2020-11-25 22:57:06,183 - INFO - Computed 0 values for constant folding\n2020-11-25 22:57:06,190 - ERROR - Tensorflow op [PartitionedCall/StaticRegexReplace: StaticRegexReplace] is not supported\n2020-11-25 22:57:06,190 - ERROR - Unsupported ops: Counter({'StaticRegexReplace': 1})\n2020-11-25 22:57:06,191 - INFO - Optimizing ONNX model\n2020-11-25 22:57:06,197 - INFO - After optimization: Identity -5 (5->0)\n2020-11-25 22:57:06,197 - INFO - \n2020-11-25 22:57:06,197 - INFO - Successfully converted TensorFlow model saved_model1 to ONNX\n2020-11-25 22:57:06,198 - INFO - ONNX model is saved at model1.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model \"saved_model1\" --inputs x:0 --outputs Identity:0 --output \"model1.onnx\""
   ]
  },
  {
   "source": [
    "Loading a model with unsupported ops into ORT raises an error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from model1.onnx failed:This is an invalid model. Error in Node:PartitionedCall/StaticRegexReplace : No Op registered for StaticRegexReplace with domain_version of 8\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "try:\n",
    "    sess = ort.InferenceSession(\"model1.onnx\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "### Enabling custom ops in the converter\n",
    "\n",
    "Fortunately, in this case there is already a custom op implementing the functionality we need: StringRegexReplace.  The converter has a rule to replace TF's StaticRegexReplace op with the StringRegexReplace custom op.  To enable conversions that use custom ops, add the `--extra_opset ai.onnx.contrib:1` flag."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-25 22:57:07.021502: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n2020-11-25 22:57:07.021939: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2020-11-25 22:57:09,686 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n2020-11-25 22:57:09.715515: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n2020-11-25 22:57:09.715879: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n2020-11-25 22:57:09.723293: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: MININT-44293CS\n2020-11-25 22:57:09.723710: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: MININT-44293CS\n2020-11-25 22:57:09.724256: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2020-11-25 22:57:09.732857: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22d3b6bcd00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-11-25 22:57:09.733136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-11-25 22:57:09,775 - INFO - Signatures found in model: [serving_default].\n2020-11-25 22:57:09,775 - WARNING - '--signature_def' not specified, using first signature: serving_default\n2020-11-25 22:57:09.777717: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n2020-11-25 22:57:09.778221: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n2020-11-25 22:57:09.785278: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n2020-11-25 22:57:09.785628: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 6 nodes (3), 5 edges (3), time = 0.453ms.\n2020-11-25 22:57:09.785995: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.008ms.\nWARNING:tensorflow:From c:\\Users\\tomwi\\OneDrive - Microsoft\\ONNX\\tensorflow-onnx\\tf2onnx\\tf_loader.py:492: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\n2020-11-25 22:57:09,804 - WARNING - From c:\\Users\\tomwi\\OneDrive - Microsoft\\ONNX\\tensorflow-onnx\\tf2onnx\\tf_loader.py:492: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\n2020-11-25 22:57:09.805797: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n2020-11-25 22:57:09.806281: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n2020-11-25 22:57:09.810686: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n2020-11-25 22:57:09.810914: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   constant_folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.512ms.\n2020-11-25 22:57:09.811065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.01ms.\n2020-11-25 22:57:09.811306: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   constant_folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.088ms.\n2020-11-25 22:57:09.811509: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\n2020-11-25 22:57:09,815 - INFO - Using tensorflow=2.3.1, onnx=1.7.0, tf2onnx=1.8.0/240803\n2020-11-25 22:57:09,815 - INFO - Using opset <onnx, 8>\n2020-11-25 22:57:09,816 - INFO - Computed 0 values for constant folding\n2020-11-25 22:57:09,821 - INFO - Optimizing ONNX model\n2020-11-25 22:57:09,829 - INFO - After optimization: Identity -5 (5->0)\n2020-11-25 22:57:09,830 - INFO - \n2020-11-25 22:57:09,830 - INFO - Successfully converted TensorFlow model saved_model1 to ONNX\n2020-11-25 22:57:09,831 - INFO - ONNX model is saved at model1.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model \"saved_model1\" --inputs x:0 --outputs Identity:0 --output \"model1.onnx\" --extra_opset ai.onnx.contrib:1"
   ]
  },
  {
   "source": [
    "### Loading custom ops into ORT\n",
    "\n",
    "Pass the location of the custom ops library into the ORT session options to use the op."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array(['Hello_World!'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from ortcustomops import get_library_path\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "so.register_custom_ops_library(get_library_path())\n",
    "\n",
    "sess = ort.InferenceSession(\"model1.onnx\", so)\n",
    "print(sess.run([\"Identity:0\"], {\"x:0\": [\"Hello World!\"]}))"
   ]
  },
  {
   "source": [
    "## Case 2: Defining new custom ops with Python\n",
    "\n",
    "If there is no existing custom op implementation, you will need to define the op yourself and add a conversion rule for it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModule2(tf.Module):\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x, segment_ids):\n",
    "        # Not supported by ONNX\n",
    "        num = tf.reduce_max(segment_ids) + 1\n",
    "        x_ = tf.strings.unsorted_segment_join(x, segment_ids, num, separator='-')\n",
    "        return x_\n",
    "\n",
    "module2 = CustomModule2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'java-script', b'car-pet'], dtype=object)>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "module2(tf.constant([\"car\", \"java\", \"pet\", \"script\"]), tf.constant([1, 0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n"
     ]
    }
   ],
   "source": [
    "concrete_fn2 = module2.__call__.get_concrete_function(tf.TensorSpec([None], tf.string), tf.TensorSpec([None], tf.int32))\n",
    "tf.saved_model.save(module2, \"saved_model2\", signatures=concrete_fn2)"
   ]
  },
  {
   "source": [
    "### Adding a custom op conversion rule using the command line\n",
    "\n",
    "We need to tell the converter how to convert the TF DecodeGif op. Even if our custom op will have the same name as the TF op, the node must be tagged with the custom ops domain `ai.onnx.contrib`.\n",
    "\n",
    "Pass `--extra_opset ai.onnx.contrib:1` and `--custom-ops DecodeGif:ai.onnx.contrib` flags to the converter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-25 22:57:10.956659: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n2020-11-25 22:57:10.956898: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2020-11-25 22:57:13,637 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n2020-11-25 22:57:13.682118: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n2020-11-25 22:57:13.682336: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n2020-11-25 22:57:13.688569: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: MININT-44293CS\n2020-11-25 22:57:13.688867: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: MININT-44293CS\n2020-11-25 22:57:13.689390: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2020-11-25 22:57:13.698055: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x21b1417f2a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-11-25 22:57:13.698321: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-11-25 22:57:13,750 - INFO - Signatures found in model: [serving_default].\n2020-11-25 22:57:13,750 - WARNING - '--signature_def' not specified, using first signature: serving_default\n2020-11-25 22:57:13.753257: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n2020-11-25 22:57:13.753783: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n2020-11-25 22:57:13.759972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n2020-11-25 22:57:13.760084: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 12 nodes (8), 12 edges (9), time = 0.553ms.\n2020-11-25 22:57:13.760362: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.011ms.\nWARNING:tensorflow:From c:\\Users\\tomwi\\OneDrive - Microsoft\\ONNX\\tensorflow-onnx\\tf2onnx\\tf_loader.py:492: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\n2020-11-25 22:57:13,786 - WARNING - From c:\\Users\\tomwi\\OneDrive - Microsoft\\ONNX\\tensorflow-onnx\\tf2onnx\\tf_loader.py:492: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\n2020-11-25 22:57:13.790346: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n2020-11-25 22:57:13.790766: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n2020-11-25 22:57:13.796279: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n2020-11-25 22:57:13.796449: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   constant_folding: Graph size after: 12 nodes (0), 12 edges (0), time = 0.612ms.\n2020-11-25 22:57:13.796628: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.01ms.\n2020-11-25 22:57:13.796792: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   constant_folding: Graph size after: 12 nodes (0), 12 edges (0), time = 0.139ms.\n2020-11-25 22:57:13.796967: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.005ms.\n2020-11-25 22:57:13,803 - INFO - Using tensorflow=2.3.1, onnx=1.7.0, tf2onnx=1.8.0/240803\n2020-11-25 22:57:13,804 - INFO - Using opset <onnx, 8>\n2020-11-25 22:57:13,805 - INFO - Computed 0 values for constant folding\n2020-11-25 22:57:13,813 - INFO - Optimizing ONNX model\n2020-11-25 22:57:13,824 - INFO - After optimization: Const -1 (2->1), Identity -6 (6->0)\n2020-11-25 22:57:13,824 - INFO - \n2020-11-25 22:57:13,824 - INFO - Successfully converted TensorFlow model saved_model2 to ONNX\n2020-11-25 22:57:13,825 - INFO - ONNX model is saved at model2a.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model \"saved_model2\" --inputs x:0 --outputs Identity:0 --output \"model2a.onnx\" --extra_opset ai.onnx.contrib:1 --custom-ops UnsortedSegmentJoin:ai.onnx.contrib"
   ]
  },
  {
   "source": [
    "### Adding a custom op conversion rule using python\n",
    "\n",
    "For more complicated conversions, the rule can be defined using python.  See the [tf2onnx repo](https://github.com/onnx/tensorflow-onnx/tree/master/tf2onnx/onnx_opset) for more conversion rule examples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "from tf2onnx import constants\n",
    "from tf2onnx.handler import tf_op\n",
    "from tf2onnx import utils\n",
    "\n",
    "# Registers a conversion rule for UnsortedSegmentJoin op\n",
    "# Rule will only be run if ai.onnx.contrib domain is included via --extra_opset flag\n",
    "@tf_op(\"UnsortedSegmentJoin\", domain=constants.CONTRIB_OPS_DOMAIN)\n",
    "class ConvertUnsortedSegmentJoinOp:\n",
    "    @classmethod\n",
    "    def version_1(cls, ctx, node, **kwargs):\n",
    "        node.type = \"MyCustomStringSegmentJoin\"\n",
    "        # Don't forget to set the domain!\n",
    "        node.domain = constants.CONTRIB_OPS_DOMAIN\n",
    "        # Ops defined using the custom ops framework only get access to inputs, not attributes\n",
    "        separator = node.get_attr_str(\"separator\") if \"separator\" in node.attr else ''\n",
    "        for a in list(node.attr.keys()):\n",
    "            del node.attr[a]\n",
    "        # Add the separator as an additional string input\n",
    "        separator_const = ctx.make_const(utils.make_name('separator_const'), np.array([separator], dtype=np.object))\n",
    "        ctx.replace_inputs(node, node.input + [separator_const.output[0]])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 14,
   "outputs": []
  },
  {
   "source": [
    "Next, call the converter using the [tf2onnx Python API](https://github.com/onnx/tensorflow-onnx#python-api-reference). All rules decorated with `@tf_op` will be used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tomwi\\OneDrive - Microsoft\\ONNX\\tensorflow-onnx\\tf2onnx\\tf_loader.py:492: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "Conversion complete!\n"
     ]
    }
   ],
   "source": [
    "from tf2onnx import tf_loader\n",
    "from tf2onnx.tfonnx import process_tf_graph\n",
    "from tf2onnx.optimizer import optimize_graph\n",
    "\n",
    "graph_def = tf_loader.from_function(concrete_fn2, input_names=['x:0'], output_names=['Identity:0'])\n",
    "extra_opset = [utils.make_opsetid(constants.CONTRIB_OPS_DOMAIN, 1)]\n",
    "with tf.Graph().as_default() as tf_graph:\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "with tf_loader.tf_session(graph=tf_graph):\n",
    "    g = process_tf_graph(tf_graph, input_names=['x:0'], output_names=['Identity:0'], extra_opset=extra_opset)\n",
    "onnx_graph = optimize_graph(g)\n",
    "model_proto = onnx_graph.make_model(\"converted\")\n",
    "utils.save_protobuf(\"model2b.onnx\", model_proto)\n",
    "print(\"Conversion complete!\")"
   ]
  },
  {
   "source": [
    "### Implementing the op in python\n",
    "\n",
    "Add a function with the `@onnx_op` decorator to register a custom op before creating the ORT InferenceSession.  The inputs will be passed in as numpy arrays, and a numpy array of the declared type should be returned.  \n",
    "\n",
    "**NOTE:** ORT only will allow an op to be registered once, so you must restart the Jupyter kernel each time you change the implementation below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ortcustomops import onnx_op, PyCustomOpDef\n",
    "import tensorflow as tf\n",
    "\n",
    "@onnx_op(op_type=\"UnsortedSegmentJoin\",\n",
    "         inputs=[PyCustomOpDef.dt_string, PyCustomOpDef.dt_int32, PyCustomOpDef.dt_int32],\n",
    "         outputs=[PyCustomOpDef.dt_string])\n",
    "def unsorted_segment_join(x, segment_ids, num_segments):\n",
    "    # The custom op implementation.\n",
    "    result = np.full([num_segments], '', dtype=np.object)\n",
    "    for s, seg_id in zip(x, segment_ids):\n",
    "        result[seg_id] += s\n",
    "    return result\n",
    "\n",
    "@onnx_op(op_type=\"MyCustomStringSegmentJoin\",\n",
    "         inputs=[PyCustomOpDef.dt_string, PyCustomOpDef.dt_int32, PyCustomOpDef.dt_int32, PyCustomOpDef.dt_string],\n",
    "         outputs=[PyCustomOpDef.dt_string])\n",
    "def string_segment_join(x, segment_ids, num_segments, separator):\n",
    "    result = [[] for i in range(num_segments)]\n",
    "    separator = separator[0]\n",
    "    for s, seg_id in zip(x, segment_ids):\n",
    "        result[seg_id].append(s)\n",
    "    result_joined = [separator.join(l) for l in result]\n",
    "    return np.array(result_joined, dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array(['javascript', 'carpet'], dtype=object)]\n[array(['java-script', 'car-pet'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from ortcustomops import get_library_path\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "so.register_custom_ops_library(get_library_path())\n",
    "\n",
    "sess = ort.InferenceSession(\"model2a.onnx\", so)\n",
    "print(sess.run([\"Identity:0\"], {\"x:0\": [\"car\", \"java\", \"pet\", \"script\"], \"segment_ids:0\": [1, 0, 1, 0]}))\n",
    "\n",
    "sess = ort.InferenceSession(\"model2b.onnx\", so)\n",
    "print(sess.run([\"Identity:0\"], {\"x:0\": [\"car\", \"java\", \"pet\", \"script\"], \"segment_ids:0\": [1, 0, 1, 0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Case 3: Implementing custom ops in C++\n",
    "\n",
    "FIXME: will do later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}